{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import open_clip\n",
    "from tqdm import tqdm\n",
    "\n",
    "def save_encoded_mnist(mnist_path, device, clip_model, clip_pretrained, save_path, batch_size):\n",
    "\n",
    "    # Load the MNIST dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((32, 32)),  # Resize to match CLIP's expected input size\n",
    "        # transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = torchvision.datasets.MNIST(root=mnist_path, train=True, download=False, transform=transform)\n",
    "    test_dataset = torchvision.datasets.MNIST(root=mnist_path, train=False, download=False, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Load the CLIP model\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        clip_model,\n",
    "        pretrained=clip_pretrained,\n",
    "        cache_dir=os.path.join(mnist_path),\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    def batch_preprocess(image_batch, preprocess):\n",
    "        to_pil_image = transforms.ToPILImage()\n",
    "        image_stack = torch.concat([preprocess(to_pil_image(img)).unsqueeze(0) for img in image_batch])\n",
    "        return image_stack\n",
    "\n",
    "    def encode_dataset(data_loader):\n",
    "        encoded_vectors = []\n",
    "        targets = []\n",
    "        images_batchs = []\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(data_loader):\n",
    "                images_preprocessed = batch_preprocess(images, preprocess)\n",
    "                images_preprocessed = images_preprocessed.to(device)\n",
    "                features = model.encode_image(images_preprocessed)\n",
    "                encoded_vectors.append(features.cpu())\n",
    "                images_batchs.append(images.cpu())\n",
    "                targets.append(labels)\n",
    "\n",
    "        encoded_vectors = torch.cat(encoded_vectors)\n",
    "        images_batchs = torch.cat(images_batchs)\n",
    "        targets = torch.cat(targets)\n",
    "\n",
    "        return encoded_vectors, targets, images_batchs\n",
    "\n",
    "    # Encode the MNIST training and test datasets\n",
    "    print(\"Begin Encoding MNIST training datasets saved.\")\n",
    "    train_encoded_vectors, train_targets, train_images_batchs = encode_dataset(train_loader)\n",
    "    print(\"Begin Encoding MNIST testing datasets saved.\")\n",
    "    test_encoded_vectors, test_targets, test_images_batchs = encode_dataset(test_loader)\n",
    "\n",
    "    # Save the encoded vectors and targets\n",
    "    torch.save((train_images_batchs, train_encoded_vectors, train_targets),\n",
    "               f'{save_path}/encoded_mnist_train_{clip_model}_{clip_pretrained}.pth')\n",
    "    torch.save((test_images_batchs, test_encoded_vectors, test_targets),\n",
    "               f'{save_path}/encoded_mnist_test_{clip_model}_{clip_pretrained}.pth')\n",
    "\n",
    "    print(\"Encoded MNIST datasets saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "open_clip_pytorch_model.bin:   0%|          | 0.00/1.71G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75e7d33e68c24c969f555939c441ca46"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Encoding MNIST training datasets saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [14:51<00:00,  7.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Encoding MNIST testing datasets saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [02:29<00:00,  7.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded MNIST datasets saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "mnist_path = '../data'\n",
    "device = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "clip_model = 'ViT-L-14'\n",
    "clip_pretrained = 'commonpool_xl_s13b_b90k'\n",
    "save_path = '../data/encoded_mnist'\n",
    "batch_size = 512\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "save_encoded_mnist(mnist_path, device, clip_model, clip_pretrained, save_path, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T16:42:45.500499Z",
     "start_time": "2024-08-01T16:22:46.487247Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import open_clip\n",
    "from tqdm import tqdm\n",
    "\n",
    "def save_encoded_usps(usps_path, device, clip_model, clip_pretrained, save_path, batch_size):\n",
    "\n",
    "    # Load the USPS dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((32, 32)),  # Resize to match CLIP's expected input size\n",
    "        # transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.USPS(root=usps_path, train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.USPS(root=usps_path, train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Load the CLIP model\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        clip_model,\n",
    "        pretrained=clip_pretrained,\n",
    "        cache_dir=os.path.join(usps_path),\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    def batch_preprocess(image_batch, preprocess):\n",
    "        to_pil_image = transforms.ToPILImage()\n",
    "        image_stack = torch.concat([preprocess(to_pil_image(img)).unsqueeze(0) for img in image_batch])\n",
    "        return image_stack\n",
    "\n",
    "    def encode_dataset(data_loader):\n",
    "        encoded_vectors = []\n",
    "        targets = []\n",
    "        images_batches = []\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(data_loader):\n",
    "                images_preprocessed = batch_preprocess(images, preprocess)\n",
    "                images_preprocessed = images_preprocessed.to(device)\n",
    "                features = model.encode_image(images_preprocessed)\n",
    "                encoded_vectors.append(features.cpu())\n",
    "                images_batches.append(images.cpu())\n",
    "                targets.append(labels)\n",
    "\n",
    "        encoded_vectors = torch.cat(encoded_vectors)\n",
    "        images_batches = torch.cat(images_batches)\n",
    "        targets = torch.cat(targets)\n",
    "\n",
    "        return encoded_vectors, targets, images_batches\n",
    "\n",
    "    # Encode the USPS training and test datasets\n",
    "    print(\"Begin Encoding USPS training datasets saved.\")\n",
    "    train_encoded_vectors, train_targets, train_images_batches = encode_dataset(train_loader)\n",
    "    print(\"Begin Encoding USPS testing datasets saved.\")\n",
    "    test_encoded_vectors, test_targets, test_images_batches = encode_dataset(test_loader)\n",
    "\n",
    "    # Save the encoded vectors and targets\n",
    "    torch.save((train_images_batches, train_encoded_vectors, train_targets),\n",
    "               f'{save_path}/encoded_usps_train_{clip_model}_{clip_pretrained}.pth')\n",
    "    torch.save((test_images_batches, test_encoded_vectors, test_targets),\n",
    "               f'{save_path}/encoded_usps_test_{clip_model}_{clip_pretrained}.pth')\n",
    "\n",
    "    print(\"Encoded USPS datasets saved.\")\n",
    "\n",
    "# Example usage:\n",
    "# save_encoded_usps('path_to_usps_data', 'cuda', 'ViT-B-32', 'openai', 'path_to_save_encoded_data', 64)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T16:43:00.989012Z",
     "start_time": "2024-08-01T16:43:00.982579Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Encoding USPS training datasets saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [01:49<00:00, 13.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Encoding USPS testing datasets saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:30<00:00, 15.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded USPS datasets saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "mnist_path = '../data'\n",
    "device = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "clip_model = 'ViT-L-14'\n",
    "clip_pretrained = 'commonpool_xl_s13b_b90k'\n",
    "save_path = '../data/encoded_usps'\n",
    "batch_size = 1024\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "save_encoded_usps(mnist_path, device, clip_model, clip_pretrained, save_path, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T16:45:41.919968Z",
     "start_time": "2024-08-01T16:43:04.570765Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "encoded_vectors, targets, images_batch = torch.load(os.path.join('../data', 'encoded_usps/encoded_usps_test_ViT-L-14_commonpool_xl_s13b_b90k.pth'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T16:50:20.039747Z",
     "start_time": "2024-08-01T16:50:20.031018Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2007, 1, 32, 32]) torch.Size([2007, 768]) torch.Size([2007])\n"
     ]
    }
   ],
   "source": [
    "print(encoded_vectors.shape, targets.shape, images_batch.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T16:50:20.685499Z",
     "start_time": "2024-08-01T16:50:20.677587Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.float32 torch.float32 torch.int64\n",
      "tensor(0.) tensor(0.9983)\n"
     ]
    }
   ],
   "source": [
    "# check the data type\n",
    "print(type(encoded_vectors), type(targets), type(images_batch))\n",
    "# check the dtype\n",
    "print(encoded_vectors.dtype, targets.dtype, images_batch.dtype)\n",
    "# give the range of encoded vectors\n",
    "print(encoded_vectors.min(), encoded_vectors.max())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T16:50:21.701039Z",
     "start_time": "2024-08-01T16:50:21.387373Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
